{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6eebc3bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T19:31:05.434598400Z",
     "start_time": "2025-10-26T19:31:00.808263200Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BSG\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Configuración inicial del entorno de entrenamiento\n",
    "─────────────────────────────────────────────────────────────\n",
    "Esta celda importa todas las librerías necesarias para:\n",
    " - Procesamiento de imágenes (OpenCV, PIL, tifffile, matplotlib)\n",
    " - Entrenamiento de modelos (PyTorch, optimizadores, métricas)\n",
    " - Visualización de resultados (seaborn, matplotlib)\n",
    " - Carga de modelos personalizados (TransformerObjectDetection)\n",
    " - Utilidades de detección y conteo (utils.counting)\n",
    " - Manejo de rutas, archivos y estructuras de carpetas\n",
    "\"\"\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Librerías del sistema y utilidades generales\n",
    "# ------------------------------------------------------------\n",
    "import os                 # Manejo del sistema de archivos y variables de entorno\n",
    "import glob               # Búsqueda de archivos por patrones\n",
    "import pathlib            # Alternativa orientada a objetos para manejo de rutas\n",
    "from pathlib import Path  # Clase Path para rutas multiplataforma\n",
    "import math               # Funciones matemáticas básicas\n",
    "import random             # Generación de números aleatorios\n",
    "import time               # Medición de tiempos de ejecución\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Librerías científicas y de manejo de datos\n",
    "# ------------------------------------------------------------\n",
    "import numpy as np               # Cálculo numérico eficiente con arreglos\n",
    "import pandas as pd              # Manipulación de datos tabulares\n",
    "import seaborn as sns            # Visualizaciones estadísticas (heatmaps, etc.)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Librerías de visualización\n",
    "# ------------------------------------------------------------\n",
    "import matplotlib.pyplot as plt             # Gráficos y figuras\n",
    "import matplotlib.patches as patches         # Dibujos sobre imágenes (rectángulos, etc.)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Librerías para procesamiento de imágenes\n",
    "# ------------------------------------------------------------\n",
    "from PIL import Image, ImageDraw  # Manipulación de imágenes (dibujar, abrir, guardar)\n",
    "from tifffile import imread       # Lectura de archivos TIFF\n",
    "import cv2                        # Procesamiento de imágenes en formato OpenCV\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Librerías de Deep Learning\n",
    "# ------------------------------------------------------------\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T  # Transformaciones de datos (rotar, normalizar, etc.)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Modelos y utilidades personalizadas del proyecto\n",
    "# ------------------------------------------------------------\n",
    "from model.transformer import TransformerObjectDetection  # Modelo Transformer personalizado\n",
    "from model.trainer import trainer                         # Función de entrenamiento principal\n",
    "import utils.counting                                     # Funciones auxiliares de inferencia\n",
    "from dstadam import DSTAdam                               # Optimizador especializado (Adam variante)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Modelos adicionales de Ultralytics (YOLO, RT-DETR)\n",
    "# ------------------------------------------------------------\n",
    "from ultralytics import YOLO     # Modelo YOLO para detección de objetos\n",
    "from ultralytics import RTDETR   # Modelo RT-DETR (transformer basado en detección rápida)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Métricas de evaluación\n",
    "# ------------------------------------------------------------\n",
    "from sklearn.metrics import (\n",
    "    roc_curve, auc, precision_recall_curve,\n",
    "    confusion_matrix, ConfusionMatrixDisplay\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56fc1ec6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T19:31:07.905718200Z",
     "start_time": "2025-10-26T19:31:07.834297600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de imágenes TIF: 2440\n",
      "Total de etiquetas TXT: 2440\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Definición de rutas de trabajo\n",
    "─────────────────────────────────────────────\n",
    "En esta celda se configuran las rutas principales del proyecto:\n",
    " - Carpeta base y subcarpetas de imágenes y etiquetas.\n",
    " - Directorios de entrenamiento y modelos.\n",
    " - Se listan los archivos disponibles para verificar integridad.\n",
    "\"\"\"\n",
    "\n",
    "# ================================================================\n",
    "# Definición de rutas base\n",
    "# ================================================================\n",
    "\n",
    "# Directorio raíz del proyecto (donde se ejecuta el notebook)\n",
    "BASE_DIR = Path(os.getcwd())\n",
    "\n",
    "# Directorios de imágenes y etiquetas de entrenamiento\n",
    "IMAGES_DIR = BASE_DIR / \"Zones_cbbox_dataset_10/train/images\"\n",
    "LABELS_DIR = BASE_DIR / \"Zones_cbbox_dataset_10/train/labels\"\n",
    "\n",
    "# Directorios de dataset completo y almacenamiento de modelos\n",
    "TRAIN_DIR = BASE_DIR / \"Zones_cbbox_dataset_10/\"\n",
    "MODEL_DIR = BASE_DIR / \"models_finetuned/bit_sted/\"\n",
    "\n",
    "# ================================================================\n",
    "# Exploración de archivos\n",
    "# ================================================================\n",
    "\n",
    "# Obtiene la lista ordenada de archivos .tif (imágenes) y .txt (etiquetas)\n",
    "imagenes = sorted(IMAGES_DIR.glob(\"*.tif\"))\n",
    "etiquetas = sorted(LABELS_DIR.glob(\"*.txt\"))\n",
    "\n",
    "# Muestra la cantidad de imágenes y etiquetas encontradas\n",
    "print(f\"Total de imágenes TIF: {len(imagenes)}\")\n",
    "print(f\"Total de etiquetas TXT: {len(etiquetas)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "725b2b48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T19:31:12.622675800Z",
     "start_time": "2025-10-26T19:31:12.592954200Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Configuración de parámetros de entrenamiento\n",
    "─────────────────────────────────────────────\n",
    "Esta celda define los hiperparámetros y ajustes generales del proceso\n",
    "de entrenamiento, incluyendo tamaño de lote, número de épocas,\n",
    "ubicación de datos y dispositivo de cómputo (CPU o GPU).\n",
    "\"\"\"\n",
    "\n",
    "# ================================================================\n",
    "# Parámetros principales del entrenamiento\n",
    "# ================================================================\n",
    "\n",
    "batch_size = 32          # Número de imágenes por lote (batch)\n",
    "epochs = 10              # Número total de épocas de entrenamiento\n",
    "\n",
    "# Directorios de datos y destino para guardar modelos\n",
    "folder_data = str(TRAIN_DIR) + \"/\"   # Carpeta raíz del dataset\n",
    "save = str(MODEL_DIR) + \"/\"          # Carpeta donde se guardarán los modelos\n",
    "\n",
    "# Número total de imágenes disponibles en el conjunto de entrenamiento\n",
    "train_size = len(sorted(IMAGES_DIR.glob(\"*.tif\")))\n",
    "\n",
    "# Selección automática del dispositivo: GPU si está disponible, de lo contrario CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Cálculo del número total de iteraciones (batches * épocas)\n",
    "iters = math.ceil(train_size / batch_size) * epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0fc09fbcecc66e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T16:20:41.010472600Z",
     "start_time": "2025-10-26T15:40:54.265608400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Inicialización del modelo, optimizador y programador de tasa de aprendizaje\n",
    "─────────────────────────────────────────────────────────────\n",
    "En esta celda se crea la instancia del modelo TransformerObjectDetection,\n",
    "se configura el optimizador DSTAdam y se define el programador (scheduler)\n",
    "para ajustar dinámicamente la tasa de aprendizaje durante el entrenamiento.\n",
    "\"\"\"\n",
    "\n",
    "# ================================================================\n",
    "# Definición del modelo Transformer personalizado\n",
    "# ================================================================\n",
    "model = TransformerObjectDetection(\n",
    "    image_size=224,       # Tamaño de entrada del modelo (224×224 píxeles)\n",
    "    N_channels=3,         # Número de canales de entrada (RGB)\n",
    "    n_model=512,          # Dimensión del espacio de características internas\n",
    "    num_blks=2,           # Número de bloques del transformador\n",
    "    device=device,        # Dispositivo de cómputo (CPU o GPU)\n",
    "    bitNet=True,          # Activación de arquitectura binarizada (Bit-STED)\n",
    "    alpha=1.0,            # Parámetro de regularización o escala interna\n",
    "    gamma=1.5             # Factor de ajuste para la función de pérdida\n",
    ")\n",
    "\n",
    "# Enviar el modelo al dispositivo seleccionado\n",
    "model.to(device)\n",
    "\n",
    "# ================================================================\n",
    "# Definición del optimizador\n",
    "# ================================================================\n",
    "optimizer = DSTAdam(\n",
    "    model.parameters(),   # Parámetros del modelo a optimizar\n",
    "    iters=iters           # Número total de iteraciones (control de tasa adaptativa)\n",
    ")\n",
    "\n",
    "# ================================================================\n",
    "# Definición del programador de tasa de aprendizaje\n",
    "# ================================================================\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',           # Reduce la LR cuando la métrica objetivo (pérdida) deja de mejorar\n",
    "    factor=0.50,          # Multiplicador de reducción (reduce LR al 50%)\n",
    "    patience=6            # Número de épocas sin mejora antes de reducir la LR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3bde2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights normal init applied to model\n",
      "The model has 6,622,088 trainable parameters\n",
      "The model size: 26.494MB\n",
      "Load train data with 2440 images\n",
      "Load val data with 1377 images\n",
      "Training for 10 epochs\n",
      " Ep:    0, Batches: 77,  Avg Loss Tr:  1.398,     Avg Time:  0.035s \n",
      "           Batches: 44,  Avg Loss Ev:  1.118,     Avg Time:  0.011s \n",
      "                                                                    Best saved at Epoch:    0\n",
      "                                                                    Epoch:    0 finished in  0.928 minutes\n",
      " Ep:    1, Batches: 77,  Avg Loss Tr:  0.962,     Avg Time:  0.025s \n",
      "           Batches: 44,  Avg Loss Ev:  0.913,     Avg Time:  0.010s \n",
      "                                                                    Best saved at Epoch:    1\n",
      " Ep:    2, Batches: 77,  Avg Loss Tr:  0.737,     Avg Time:  0.024s \n",
      "           Batches: 44,  Avg Loss Ev:  0.808,     Avg Time:  0.010s \n",
      "                                                                    Best saved at Epoch:    2\n",
      " Ep:    3, Batches: 77,  Avg Loss Tr:  0.689,     Avg Time:  0.024s \n",
      "           Batches: 44,  Avg Loss Ev:  0.808,     Avg Time:  0.010s \n",
      " Ep:    4, Batches: 77,  Avg Loss Tr:  0.661,     Avg Time:  0.025s \n",
      "           Batches: 44,  Avg Loss Ev:  0.801,     Avg Time:  0.010s \n",
      "                                                                    Best saved at Epoch:    4\n",
      " Ep:    5, Batches: 77,  Avg Loss Tr:  0.632,     Avg Time:  0.026s \n",
      "           Batches: 44,  Avg Loss Ev:  0.677,     Avg Time:  0.010s \n",
      "                                                                    Best saved at Epoch:    5\n",
      " Ep:    6, Batches: 77,  Avg Loss Tr:  0.617,     Avg Time:  0.024s \n",
      "           Batches: 44,  Avg Loss Ev:  0.811,     Avg Time:  0.010s \n",
      " Ep:    7, Batches: 77,  Avg Loss Tr:  0.643,     Avg Time:  0.024s \n",
      "           Batches: 44,  Avg Loss Ev:  0.721,     Avg Time:  0.010s \n",
      " Ep:    8, Batches: 77,  Avg Loss Tr:  0.650,     Avg Time:  0.024s \n",
      "           Batches: 44,  Avg Loss Ev:  0.697,     Avg Time:  0.010s \n",
      " Ep:    9, Batches: 77,  Avg Loss Tr:  0.621,     Avg Time:  0.023s \n",
      "           Batches: 44,  Avg Loss Ev:  0.710,     Avg Time:  0.010s \n",
      "                                                                     Epoch:    9 finished in  2.759 minutes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerObjectDetection(\n",
       "  (encoder1): Encoder(\n",
       "    (patch_embedding): PatchEmbedding(\n",
       "      (ln1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (proj): Linear(in_features=192, out_features=256, bias=True)\n",
       "      (activation): GELU(approximate='none')\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (blks): ModuleList(\n",
       "      (0-1): 2 x Block_encoder(\n",
       "        (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): BitMGQA(\n",
       "          (fc_q): BitLinear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): BitLinear(in_features=256, out_features=128, bias=True)\n",
       "          (fc_v): BitLinear(in_features=256, out_features=128, bias=True)\n",
       "          (fc_o): BitLinear(in_features=128, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ffn): BitFeedForward(\n",
       "          (ff): Sequential(\n",
       "            (0): BitLinear(in_features=256, out_features=512, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (3): Dropout(p=0.1, inplace=False)\n",
       "            (4): BitLinear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder2): Encoder(\n",
       "    (patch_embedding): PatchEmbedding(\n",
       "      (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (proj): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (activation): GELU(approximate='none')\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (blks): ModuleList(\n",
       "      (0-1): 2 x Block_encoder(\n",
       "        (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): BitMGQA(\n",
       "          (fc_q): BitLinear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_k): BitLinear(in_features=512, out_features=256, bias=True)\n",
       "          (fc_v): BitLinear(in_features=512, out_features=256, bias=True)\n",
       "          (fc_o): BitLinear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ffn): BitFeedForward(\n",
       "          (ff): Sequential(\n",
       "            (0): BitLinear(in_features=512, out_features=1024, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (3): Dropout(p=0.1, inplace=False)\n",
       "            (4): BitLinear(in_features=1024, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_final1): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "    )\n",
       "    (2): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (detection1): DetectionCircle(\n",
       "    (diou): BboxLoss()\n",
       "    (fl): FocalLoss()\n",
       "  )\n",
       "  (upsample): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "    )\n",
       "    (1): Upsample(scale_factor=2.0, mode='nearest')\n",
       "  )\n",
       "  (conv_final2): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "    )\n",
       "    (2): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (detection2): DetectionCircle(\n",
       "    (diou): BboxLoss()\n",
       "    (fl): FocalLoss()\n",
       "  )\n",
       "  (loss_m): DiceLoss()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Ejecución del entrenamiento del modelo\n",
    "─────────────────────────────────────────────\n",
    "En esta celda se llama a la función `trainer`, que ejecuta el ciclo\n",
    "completo de entrenamiento del modelo TransformerObjectDetection.\n",
    "Se incluyen los parámetros principales como el optimizador, el\n",
    "scheduler y las rutas para guardar los resultados.\n",
    "\"\"\"\n",
    "\n",
    "# ================================================================\n",
    "# Ejecución del proceso de entrenamiento\n",
    "# ================================================================\n",
    "\n",
    "trainer(\n",
    "    model,                  # Modelo a entrenar (instancia de TransformerObjectDetection)\n",
    "    folder_data,            # Ruta base del dataset (imágenes y etiquetas)\n",
    "    None,                   # Parámetro reservado (por ejemplo, validación o dataset extra)\n",
    "    optimizer,              # Optimizador definido previamente (DSTAdam)\n",
    "    scheduler,              # Programador de tasa de aprendizaje (ReduceLROnPlateau)\n",
    "    epochs=epochs,          # Número de épocas de entrenamiento\n",
    "    use_scheduler_dict=True,# Habilita el uso del scheduler dinámico\n",
    "    device=device,          # Dispositivo de cómputo (CPU o GPU)\n",
    "    folder_save_results=save,# Carpeta donde se guardarán resultados y pesos\n",
    "    batch_size=batch_size   # Tamaño del lote por iteración\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language": "python",
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
